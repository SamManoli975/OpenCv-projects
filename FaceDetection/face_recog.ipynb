{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Load and process an example image\u001b[39;00m\n\u001b[0;32m     78\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknown_faces/sam.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 80\u001b[0m processed_face \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_and_align_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processed_face \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# Show the final processed face image\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     cv\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed Face\u001b[39m\u001b[38;5;124m\"\u001b[39m, processed_face)\n",
      "Cell \u001b[1;32mIn[28], line 22\u001b[0m, in \u001b[0;36mdetect_and_align_face\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Detect faces using RetinaFace\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_faces_with_retina\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo faces detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m, in \u001b[0;36mdetect_faces_with_retina\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_faces_with_retina\u001b[39m(image_path):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Detect faces using RetinaFace from the image path\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mRetinaFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\samma\\anaconda3\\envs\\env1\\lib\\site-packages\\retinaface\\RetinaFace.py:96\u001b[0m, in \u001b[0;36mdetect_faces\u001b[1;34m(img_path, threshold, model, allow_upscaling)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m    100\u001b[0m nms_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\samma\\anaconda3\\envs\\env1\\lib\\site-packages\\retinaface\\RetinaFace.py:54\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m model  \u001b[38;5;66;03m# singleton design pattern\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[0;32m     53\u001b[0m     model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m---> 54\u001b[0m         \u001b[43mretinaface_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     55\u001b[0m         input_signature\u001b[38;5;241m=\u001b[39m(tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m3\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),),\n\u001b[0;32m     56\u001b[0m     )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\samma\\anaconda3\\envs\\env1\\lib\\site-packages\\retinaface\\model\\retinaface_model.py:1027\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1019\u001b[0m ssh_m3_det_conv1_bn \u001b[38;5;241m=\u001b[39m BatchNormalization(\n\u001b[0;32m   1020\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.9999999494757503e-05\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssh_m3_det_conv1_bn\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m )(ssh_m3_det_conv1)\n\u001b[0;32m   1023\u001b[0m ssh_m3_det_context_conv1_bn \u001b[38;5;241m=\u001b[39m BatchNormalization(\n\u001b[0;32m   1024\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.9999999494757503e-05\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssh_m3_det_context_conv1_bn\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m )(ssh_m3_det_context_conv1)\n\u001b[1;32m-> 1027\u001b[0m x1_shape \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mssh_c3_up\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1028\u001b[0m x2_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(ssh_c2_lateral_relu)\n\u001b[0;32m   1029\u001b[0m offsets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, (x1_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m x2_shape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, (x1_shape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m x2_shape[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\samma\\anaconda3\\envs\\env1\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\samma\\anaconda3\\envs\\env1\\lib\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "from retinaface import RetinaFace\n",
    "import numpy as np\n",
    "\n",
    "# Function to detect faces with RetinaFace using an image path\n",
    "def detect_faces_with_retina(image_path):\n",
    "    # Detect faces using RetinaFace from the image path\n",
    "    resp = RetinaFace.detect_faces(img_path = image_path)\n",
    "    return resp\n",
    "\n",
    "# Function to detect and align face using RetinaFace and Mediapipe\n",
    "def detect_and_align_face(image_path):\n",
    "    # Load the image\n",
    "    image = cv.imread(image_path)\n",
    "    \n",
    "    if image is None:\n",
    "        print(\"Failed to load image\")\n",
    "        return None\n",
    "\n",
    "    # Detect faces using RetinaFace\n",
    "    faces = detect_faces_with_retina(image_path)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        print(\"No faces detected.\")\n",
    "        return None\n",
    "    \n",
    "    # Get the first face's bounding box from RetinaFace\n",
    "    for face_id, face_info in faces.items():\n",
    "        (x1, y1, x2, y2) = face_info['facial_area']\n",
    "        face = image[y1:y2, x1:x2]\n",
    "        \n",
    "        # Now use Mediapipe for facial landmarks\n",
    "        with mp.solutions.face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5) as face_mesh:\n",
    "            # Convert to RGB for Mediapipe\n",
    "            rgb_image = cv.cvtColor(face, cv.COLOR_BGR2RGB)\n",
    "            face_mesh_results = face_mesh.process(rgb_image)\n",
    "\n",
    "            if face_mesh_results.multi_face_landmarks:\n",
    "                landmarks = face_mesh_results.multi_face_landmarks[0].landmark\n",
    "                \n",
    "                # Approximate eye positions based on Mediapipe landmarks (indices for left and right eyes)\n",
    "                left_eye = np.array([int(landmarks[33].x * (x2 - x1)), int(landmarks[33].y * (y2 - y1))])\n",
    "                right_eye = np.array([int(landmarks[263].x * (x2 - x1)), int(landmarks[263].y * (y2 - y1))])\n",
    "\n",
    "                print(f\"Left eye position: {left_eye}\")\n",
    "                print(f\"Right eye position: {right_eye}\")\n",
    "\n",
    "                # Align face based on eye positions\n",
    "                aligned_face = align_face(face, left_eye, right_eye)\n",
    "                \n",
    "                # Resize the aligned face to a fixed size\n",
    "                resized_face = cv.resize(aligned_face, (128, 128))\n",
    "\n",
    "                return resized_face\n",
    "            \n",
    "    return None\n",
    "\n",
    "# Function to align the face based on eye positions\n",
    "def align_face(image, left_eye, right_eye):\n",
    "    # Calculate the angle between the eyes\n",
    "    dY = right_eye[1] - left_eye[1]\n",
    "    dX = right_eye[0] - left_eye[0]\n",
    "    angle = np.degrees(np.arctan2(dY, dX))\n",
    "\n",
    "    # Calculate the center point between the eyes\n",
    "    eyes_center = ((left_eye[0] + right_eye[0]) // 2, (left_eye[1] + right_eye[1]) // 2)\n",
    "\n",
    "    # Get the rotation matrix for rotating and aligning the face\n",
    "    M = cv.getRotationMatrix2D(eyes_center, angle, scale=1)\n",
    "\n",
    "    # Perform the affine transformation (rotate the image)\n",
    "    aligned_face = cv.warpAffine(image, M, (image.shape[1], image.shape[0]), flags=cv.INTER_CUBIC)\n",
    "    \n",
    "    return aligned_face\n",
    "\n",
    "# Load and process an example image\n",
    "image_path = \"known_faces/sam.jpg\"\n",
    "\n",
    "processed_face = detect_and_align_face(image_path)\n",
    "\n",
    "if processed_face is not None:\n",
    "    # Show the final processed face image\n",
    "    cv.imshow(\"Processed Face\", processed_face)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()\n",
    "else:\n",
    "    print(\"No face detected\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
